@abnt-options{ABNT-final,
abnt-emphasize={\textbf}
}


@article {vinyals_grandmaster_2019,
	title = {Grandmaster level in {StarCraft} {II} using multi-agent reinforcement learning},
	volume = {575},
	issn = {0028-0836, 1476-4687},
	url = {http://www.nature.com/articles/s41586-019-1724-z},
	doi = {10.1038/s41586-019-1724-z},
	language = {en},
	number = {7782},
	urldate = {2021-08-29},
	journal = {Nature},
	author = {Vinyals, Oriol and Babuschkin, Igor and Czarnecki, Wojciech M. and Mathieu, Michaël and Dudzik, Andrew and Chung, Junyoung and Choi, David H. and Powell, Richard and Ewalds, Timo and Georgiev, Petko and Oh, Junhyuk and Horgan, Dan and Kroiss, Manuel and Danihelka, Ivo and Huang, Aja and Sifre, Laurent and Cai, Trevor and Agapiou, John P. and Jaderberg, Max and Vezhnevets, Alexander S. and Leblond, Rémi and Pohlen, Tobias and Dalibard, Valentin and Budden, David and Sulsky, Yury and Molloy, James and Paine, Tom L. and Gulcehre, Caglar and Wang, Ziyu and Pfaff, Tobias and Wu, Yuhuai and Ring, Roman and Yogatama, Dani and Wünsch, Dario and McKinney, Katrina and Smith, Oliver and Schaul, Tom and Lillicrap, Timothy and Kavukcuoglu, Koray and Hassabis, Demis and Apps, Chris and Silver, David},
	month = nov,
	year = {2019},
	pages = {350--354},
}

@Article{starcraft_macromanagement_2021,
    AUTHOR = {Huang, Wenzhen and Yin, Qiyue and Zhang, Junge and Huang, Kaiqi},
    TITLE = {Learning Macromanagement in Starcraft by Deep Reinforcement Learning},
    JOURNAL = {Sensors},
    VOLUME = {21},
    YEAR = {2021},
    NUMBER = {10},
    ARTICLE-NUMBER = {3332},
    URL = {https://www.mdpi.com/1424-8220/21/10/3332},
    PubMedID = {34065012},
    ISSN = {1424-8220},
    ABSTRACT = {StarCraft is a real-time strategy game that provides a complex environment for AI research. Macromanagement, i.e., selecting appropriate units to build depending on the current state, is one of the most important problems in this game. To reduce the requirements for expert knowledge and enhance the coordination of the systematic bot, we select reinforcement learning (RL) to tackle the problem of macromanagement. We propose a novel deep RL method, Mean Asynchronous Advantage Actor-Critic (MA3C), which computes the approximate expected policy gradient instead of the gradient of sampled action to reduce the variance of the gradient, and encode the history queue with recurrent neural network to tackle the problem of imperfect information. The experimental results show that MA3C achieves a very high rate of winning, approximately 90\%, against the weaker opponents and it improves the win rate about 30\% against the stronger opponents. We also propose a novel method to visualize and interpret the policy learned by MA3C. Combined with the visualized results and the snapshots of games, we find that the learned macromanagement not only adapts to the game rules and the policy of the opponent bot, but also cooperates well with the other modules of MA3C-Bot.},
    DOI = {10.3390/s21103332}
}

@misc{han_tstarbotx_2021,
      title={TStarBot-X: An Open-Sourced and Comprehensive Study for Efficient League Training in StarCraft II Full Game}, 
      author={Lei Han and Jiechao Xiong and Peng Sun and Xinghai Sun and Meng Fang and Qingwei Guo and Qiaobo Chen and Tengfei Shi and Hongsheng Yu and Xipeng Wu and Zhengyou Zhang},
      year={2021},
      eprint={2011.13729},
      archivePrefix={arXiv},
      primaryClass={cs.AI}
}

